# ğŸ“ˆ Real-Time Stock Data Pipeline using Kafka and AWS

This project simulates a real-time stock market data pipeline using **Apache Kafka**, **AWS S3**, **AWS Glue**, and **Amazon Athena**. Stock data is streamed from a CSV file, processed using Kafka, and then sent to S3 for querying with SQL.

---

## ğŸš€ Architecture

![Architecture Diagram](architecture.png)

---

## ğŸ§© Components Overview

| Component         | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| ğŸ Python App     | Simulates real-time stock data using a CSV file                            |
| ğŸ§ª Kafka Producer | Sends simulated data to Kafka topics                                       |
| âš™ï¸ Kafka Broker   | Hosted on Amazon EC2                                                       |
| ğŸ§² Kafka Consumer | Reads data from Kafka and writes to Amazon S3                             |
| ğŸª£ Amazon S3      | Stores raw stock data in object format                                     |
| ğŸ•·ï¸ AWS Glue       | Crawls S3 bucket and updates the Glue Data Catalog                         |
| ğŸ” Amazon Athena  | Allows SQL queries over structured S3 data through Glue Data Catalog       |

---

## ğŸ› ï¸ Technologies Used

- Python 3.x
- Apache Kafka (`kafka-python`)
- Amazon EC2 (Kafka Broker hosting)
- Amazon S3 (Storage)
- AWS Glue (Data Catalog & Crawler)
- Amazon Athena (SQL over S3)
- Boto3 (for optional S3 automation)

---

## ğŸ“ Project Structure

```
real-time-stock-data-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ architecture.png           # Project diagram
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ stocks.csv             # Sample dataset
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ producer.ipynb         # Notebook version
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ consumer/
â”‚   â”œâ”€â”€ consumer.ipynb
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ s3_bucket_setup.md
â”‚   â”œâ”€â”€ glue_crawler_config.md
â”‚   â”œâ”€â”€ athena_queries.sql
â”‚   â””â”€â”€ boto3_s3_upload.py     # Optional boto3 upload
```

---

## ğŸ”§ Setup Instructions

### ğŸ“¦ 1. Clone the Repository
```bash
git clone https://github.com/yourusername/real-time-stock-data-pipeline.git
cd real-time-stock-data-pipeline
```

### ğŸ 2. Install Python Dependencies
Navigate to `producer/` and `consumer/` folders individually:
```bash
pip install -r requirements.txt
```

> If you're using Jupyter Notebook, install using notebook interface or VSCode kernel.

---

### ğŸ“¤ 3. Kafka Setup on EC2
- Launch EC2 instance (Ubuntu preferred)
- Install Java & Kafka
- Start Kafka and Zookeeper
- Open necessary ports in security group (9092, 2181)
- Update your Kafka producer & consumer bootstrap server to EC2 IP

---

### â–¶ï¸ 4. Run the Pipeline

#### Run Producer
```bash
cd producer
jupyter notebook
# Open producer.ipynb and run all cells
```

#### Run Consumer
```bash
cd consumer
jupyter notebook
# Open consumer.ipynb and run all cells
```

This will start sending data from the CSV to Kafka â†’ and saving to S3.

---

### ğŸ§° 5. Configure AWS Services

#### ğŸª£ S3
- Create a new bucket (e.g., `stock-data-bucket`)
- Ensure correct region and permissions (write access from consumer)

#### ğŸ•·ï¸ Glue
- Create a Crawler to scan your S3 folder
- Output to Glue Data Catalog
- Run the Crawler after some data is pushed

#### ğŸ” Athena
- Open Athena console
- Select database created by Glue
- Start querying data using:
```sql
SELECT * FROM stock_data LIMIT 10;
```

---

## ğŸ“ Sample Athena Query
```sql
SELECT symbol, AVG(price) as avg_price
FROM stock_data
WHERE event_time > current_date - interval '7' day
GROUP BY symbol
ORDER BY avg_price DESC;
```

---

## ğŸ“š Useful References
- [AWS Glue Docs](https://docs.aws.amazon.com/glue/)
- [Amazon Athena Docs](https://docs.aws.amazon.com/athena/)
- [Kafka Python Client](https://kafka-python.readthedocs.io/en/master/)

---

## ğŸ‘¨â€ğŸ’» Author
**Mohammad Mufeez**  
BSc CS Student @ FAST NUCES | Data Engineering Enthusiast  
[GitHub Profile](https://github.com/yourusername)

---

## ğŸ“„ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
